{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: VAE\n",
    "\n",
    "The code snippets below implement a VAE for MNIST digits and some visualizations for the results. Check the pdf for instructions of what do to.\n",
    "\n",
    "## Model definition and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e27926d8d1e4b8faad15a25393c6628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyperparameters of the model and optimization\n",
    "K = 5\n",
    "obs_sigma = 0.1\n",
    "batch_size = 50\n",
    "# You will want to use a bigger number, but I set it small by default\n",
    "# so that it is faster to run the code for the first time. Increasing\n",
    "# numEpoch does not yet count as proper modification.\n",
    "numEpoch = 5   \n",
    "lr = 0.001\n",
    "\n",
    "# MNIST data \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prior distribution for latent variables\n",
    "p_z = torch.distributions.Normal(0., 1.)\n",
    "\n",
    "# Encoder and decoder specifications\n",
    "D = 28*28\n",
    "H = 20\n",
    "encoder_mu = nn.Sequential(nn.Linear(D,H), nn.ReLU(),\n",
    "                           nn.Linear(H,H), nn.ReLU(),\n",
    "                           nn.Linear(H,K,bias=True))\n",
    "encoder_sigma = nn.Sequential(nn.Linear(D,H), nn.ReLU(),\n",
    "                              nn.Linear(H,H), nn.ReLU(),\n",
    "                              nn.Linear(H,K,bias=True))\n",
    "decoder = nn.Sequential(nn.Linear(K,H), nn.ReLU(),\n",
    "                        nn.Linear(H,H), nn.ReLU(),\n",
    "                        nn.Linear(H,D,bias=True))\n",
    "\n",
    "# Optimize over parameters of all networks\n",
    "params = list(encoder_mu.parameters()) + list(encoder_sigma.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "elbos = []\n",
    "for i in tqdm(range(numEpoch)):\n",
    "    batches = iter(train_loader)\n",
    "\n",
    "    epochloss = 0.\n",
    "    for j in range(len(batches)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Next batch of samples\n",
    "        batch_data, batch_targets = next(batches)\n",
    "        x = batch_data.reshape((batch_size,-1))\n",
    "    \n",
    "        # Form parameters of approximation\n",
    "        mu = encoder_mu(x)\n",
    "        unconstrained_sigma = encoder_sigma(x)\n",
    "        sigma = torch.sigmoid(unconstrained_sigma)\n",
    "        \n",
    "        # Sample from approximation\n",
    "        # - rsample() handles reparameterization internally,\n",
    "        #   so we do not need to do it manually\n",
    "        # - Note that sample() would not work correctly\n",
    "        q_z_x = torch.distributions.Normal(mu, sigma)\n",
    "        z = q_z_x.rsample()\n",
    "\n",
    "        # Find mean parameters of observed data\n",
    "        x_mean = decoder(z)\n",
    "    \n",
    "        # Leaning objective\n",
    "        # - Sum over the columns to handle multivariate distributions\n",
    "        # - Mean over the rows as we want expected loss per data point (not sum)\n",
    "        logp_x_z = torch.sum(torch.distributions.Normal(x_mean, obs_sigma).log_prob(x), 1)\n",
    "        KL = torch.sum(q_z_x.log_prob(z) - p_z.log_prob(z), 1)\n",
    "        loss = - torch.mean(logp_x_z - KL, 0)\n",
    "        epochloss += loss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elbos.append(-epochloss/len(batches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.plot(elbos)\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('ELBO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "# Note: Uses the values from the last iteration of the algorithm\n",
    "for sam in range(8):\n",
    "    plt.subplot(4,4,sam*2+1)\n",
    "    plt.imshow(x[sam,:].reshape(28,28))\n",
    "\n",
    "    plt.subplot(4,4,sam*2+2)\n",
    "    plt.imshow(x_mean[sam,:].detach().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ])),\n",
    "  batch_size=10000, shuffle=True)\n",
    "\n",
    "batches = iter(train_loader)\n",
    "batch_data, batch_targets = next(batches)\n",
    "x = batch_data.reshape((10000,-1))\n",
    "\n",
    "mu = encoder_mu(x)\n",
    "unconstrained_sigma = encoder_sigma(x)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "for c in range(10):\n",
    "    _ = plt.plot(mu.detach()[batch_targets==c,0], mu.detach()[batch_targets==c,1], '.', alpha=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
